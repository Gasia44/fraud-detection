{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(model, X, y):\n",
    "    \"\"\"\n",
    "    calulate precision, recall based on different thresholds\n",
    "    :param X: pandas dataframe\n",
    "    :param y: array of labels\n",
    "    :return: dataframe of model_name, precision, recall, threshold\n",
    "    \"\"\"\n",
    "    thresh_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    predd = model.predict_proba(X)\n",
    "    for my_threshold in range(500, 1000, 10):\n",
    "        thres = my_threshold / 1000\n",
    "        y_test_pred = (predd >= thres).argmax(axis=1)\n",
    "\n",
    "        recall1 = round(recall_score(y, y_test_pred), 2)\n",
    "        prec1 = round(precision_score(y, y_test_pred), 2)\n",
    "        precision_list.append(prec1)\n",
    "        recall_list.append(recall1)\n",
    "        thresh_list.append(thres)\n",
    "\n",
    "    df_threshold = pd.DataFrame(\n",
    "        {'precision_fraud': precision_list, 'recall_fraud': recall_list,\n",
    "         'threshold': thresh_list})\n",
    "    return df_threshold\n",
    "\n",
    "def report_optimal_threshold(model, X, y):\n",
    "    \"\"\"\n",
    "    calulate the optimal threshold for balanced precision and recall\n",
    "    :param X: pandas dataframe\n",
    "    :param y: array of labels\n",
    "    :return: float of optimal threshold\n",
    "    \"\"\"\n",
    "    df_threshold = calculate_precision_recall(model, X, y)\n",
    "    df_threshold['diff'] = np.abs(df_threshold['precision_fraud'] - df_threshold['recall_fraud'])\n",
    "    optimal_threshold = df_threshold[df_threshold['diff'] == df_threshold['diff'].min()]['threshold'].values[0]\n",
    "    return optimal_threshold\n",
    "\n",
    "def calculate_precision_threshold(model, X, y):\n",
    "    df_threshold = calculate_precision_recall(model, X, y)\n",
    "\n",
    "    predd = model.predict_proba(X)\n",
    "    list_of_met = []\n",
    "    for i in [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.98]:\n",
    "        df_threshold['prec_th'] = i\n",
    "        list_of_met.append(df_threshold[df_threshold['precision_fraud'] >= i][:1])\n",
    "\n",
    "    df_met = pd.concat(list_of_met)\n",
    "    conf_matrix_all = [list(report_confusion_matrix(y, predd, threshold=prec_th)) for prec_th in\n",
    "                       df_met['threshold']]\n",
    "    df_res = pd.DataFrame(conf_matrix_all, \\\n",
    "                          columns=[\"NonFraud_ActualNonFraud\", \"Fraud_ActualNonFraud\", \"NonFraud_ActualFraud\",\n",
    "                                   \"Fraud_ActualFraud\"])\n",
    "\n",
    "    df_met['Fraud_ActualFraud'] = df_res['Fraud_ActualFraud'].values\n",
    "    df_met['Fraud_ActualNonFraud'] = df_res['Fraud_ActualNonFraud'].values\n",
    "    df_met['NonFraud_ActualFraud'] = df_res['NonFraud_ActualFraud'].values\n",
    "    df_met['NonFraud_ActualNonFraud'] = df_res['NonFraud_ActualNonFraud'].values\n",
    "\n",
    "    df_met = df_met.reset_index(drop=True)\n",
    "    return df_met\n",
    "\n",
    "\n",
    "def report_confusion_matrix(y, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param y: true labels\n",
    "    :param y_pred_proba: predicted probability\n",
    "    :param threshold: threshold of fraud\n",
    "    :return: 1d numpy array of confusion matrix\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).argmax(axis=1)\n",
    "    confusion_mat = confusion_matrix(y, y_pred).ravel()\n",
    "    return confusion_mat\n",
    "\n",
    "def report_metrics(model, X, y, optimal_threshold=None, data_name=None):\n",
    "    \"\"\"\n",
    "    calculate confusion matrix, roc_auc, precision and recall\n",
    "    :param X: dataframe ready to predcit\n",
    "    :param y: true labels\n",
    "    :param data_name: string name of data (train, test)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X)\n",
    "    roc_score = roc_auc_score(y, y_pred_proba[:, 1], average=\"weighted\")\n",
    "\n",
    "    curr_threshold = 0.96\n",
    "    conf_matrix_ = list(report_confusion_matrix(y, y_pred_proba, threshold=curr_threshold))\n",
    "    conf_matrix_.append(curr_threshold)\n",
    "    # threshold IsOptimal\n",
    "    conf_matrix_.append(False)\n",
    "\n",
    "    if not optimal_threshold:\n",
    "        optimal_threshold = report_optimal_threshold(model, X, y)\n",
    "    conf_matrix_optimal_th = list(report_confusion_matrix(y, y_pred_proba, threshold=optimal_threshold))\n",
    "    conf_matrix_optimal_th.append(optimal_threshold)\n",
    "    # threshold IsOptimal\n",
    "    conf_matrix_optimal_th.append(True)\n",
    "\n",
    "    df_res = pd.DataFrame([conf_matrix_, conf_matrix_optimal_th], \\\n",
    "                          columns=[\"NonFraud_ActualNonFraud\", \"Fraud_ActualNonFraud\", \\\n",
    "                                   \"NonFraud_ActualFraud\", \"Fraud_ActualFraud\", \\\n",
    "                                   \"threshold\", \"optimal_threshold\"])\n",
    "    df_res['roc_auc'] = roc_score\n",
    "    df_res['data'] = data_name\n",
    "    df_res['user_count'] = len(y)\n",
    "\n",
    "    df_res['precision'] = df_res['Fraud_ActualFraud'] / (\n",
    "            df_res['Fraud_ActualFraud'] + df_res['Fraud_ActualNonFraud'])\n",
    "    df_res['recall'] = df_res['Fraud_ActualFraud'] / (df_res['Fraud_ActualFraud'] + df_res['NonFraud_ActualFraud'])\n",
    "    df_res['f1_score'] = (2 * df_res['precision'] * df_res['recall']) / (df_res['precision'] + df_res['recall'])\n",
    "\n",
    "    df_res = round(df_res, 3)\n",
    "    df_res = df_res[['data', 'user_count', 'threshold', 'optimal_threshold', \\\n",
    "                     'roc_auc', 'precision', 'recall', 'f1_score', \\\n",
    "                     'Fraud_ActualFraud', 'Fraud_ActualNonFraud', 'NonFraud_ActualFraud',\n",
    "                     'NonFraud_ActualNonFraud']]\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def report_metric(model, x_train, y_train, x_test, y_test):\n",
    "    train_report = report_metrics(model, x_train, y_train, data_name='train')\n",
    "    opt_t=train_report[train_report['optimal_threshold'] ==True]['threshold'].values[0]\n",
    "    test_report= report_metrics(model, x_test, y_test, optimal_threshold=opt_t, data_name='test')\n",
    "\n",
    "    report = pd.concat([train_report, test_report])\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correlated_features(df_merged_filtered):\n",
    "    \"\"\"\n",
    "    filter correlated features for multicolinearity problem\n",
    "    :param df_merged_filtered:\n",
    "    :return: dataframe wxluded the correlated features\n",
    "    \"\"\"\n",
    "    columns_keep = []\n",
    "    for col in df_merged_filtered.columns:\n",
    "        if df_merged_filtered[col].nunique() > 1:\n",
    "            columns_keep.append(col)\n",
    "\n",
    "    df_merged_filtered = df_merged_filtered[columns_keep]\n",
    "    not_dummy = ['impression_col', 'session_click', 'session_dev', 'session_ip', 'session_post_back', \\\n",
    "                 'device_plat_unique_vals', 'profile_change_fieldid', 'question_diff_source', \\\n",
    "                 'survey_status_', 'ss1_provider_', 'ss1_provider_', 'ss2_provider_', 'ss3_provider_', \\\n",
    "                 'act_conv_rate_all_', 'success_rate_provider_', 'status_change_reasonid_', \\\n",
    "                 'session_length_minutes_', '_browser_unique_vals', 'avg_acloi_provider_', 'sum_acloi_provider_']\n",
    "\n",
    "    pre_dummy_variables_to_add = [col for col in df_merged_filtered.columns if (\n",
    "            (df_merged_filtered[col].nunique() == 2) & (~((\"CLICK\" in col) | (\"BACK\" in col))))]\n",
    "    dummy_variables_to_add = [x for x in pre_dummy_variables_to_add if all(elem not in x for elem in not_dummy)]\n",
    "\n",
    "    variables_to_filter = list(set(df_merged_filtered.columns).difference(set(dummy_variables_to_add)))\n",
    "\n",
    "    df_temp = df_merged_filtered[variables_to_filter]\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df_temp.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find index of feature columns with correlation greater than 0.85\n",
    "    to_drop = [column for column in upper.columns if any((upper[column] > 0.90))]\n",
    "    print('dropping #features:', len(to_drop))\n",
    "    df_temp = df_temp.drop(to_drop, axis=1)\n",
    "    df_merged_filtered = pd.concat([df_temp, df_merged_filtered[dummy_variables_to_add]], axis=1)\n",
    "    print('filtering correlated features done.')\n",
    "    return df_merged_filtered\n",
    "\n",
    "\n",
    "def adjust_features(_features, X):\n",
    "    \"\"\"\n",
    "        adjust the features to the trained set (sequence and existence)\n",
    "        :param X: pandas dataframe\n",
    "        :return: adjusted dataframe\n",
    "        \"\"\"\n",
    "    cols_to_add = set(_features).difference(set(X.columns))\n",
    "    df_cols_to_add = pd.DataFrame(columns=cols_to_add)\n",
    "    df_to_predict_ready = pd.concat([X, df_cols_to_add], axis=1).fillna(0)\n",
    "    x_adjusted = df_to_predict_ready[_features]\n",
    "    return x_adjusted\n",
    "\n",
    "def standardize_data(df, scaler=None):\n",
    "    \"\"\"\n",
    "    standardize data based on Z-score\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        print(\"scaler is none\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "    return df_scaled, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sj = pd.read_csv('../data/processed/processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sj.loc[:, ~df_sj.columns.isin([\"user_id\", \"fraud\"])]\n",
    "y = df_sj['fraud'].values\n",
    "X = X.replace(math.inf, 0)\n",
    "x_train_raw, x_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_filt = filter_correlated_features(x_train_raw)\n",
    "x_train, scaler = standardize_data(x_train_filt)\n",
    "\n",
    "x_test_filt = adjust_features(x_train.columns, x_test_raw)\n",
    "x_test, _ = standardize_data(x_test_filt, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(x_train, y_train, penalty_v = \"l1\", C_value = 0.01, \\\n",
    "                            GridSearch = False, max_iter = 1000):\n",
    "    \n",
    "    pars = {\"random_state\": 42, \"class_weight\": \"balanced\", \"n_jobs\": -1, \"solver\": \"saga\", \"max_iter\": 10000,\n",
    "               \"C\": C_value, \"verbose\": 0, 'penalty': penalty_v}\n",
    "    lr_model = LogisticRegression(**pars)\n",
    "    \n",
    "    if GridSearch:\n",
    "        parameters = {\"C\": [0.001, 0.01, 0.1, 1], 'penalty':['l1', 'l2']}\n",
    "        model_logit = GridSearchCV(lr_model, parameters, scoring=\"roc_auc\", cv=None, verbose=5,\n",
    "                                   pre_dispatch=20, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "        model_logit.fit(x_train,y_train)\n",
    "        print(\"GridSearch best parameters: {}\".format(model_logit.best_params_))\n",
    "        \n",
    "        C_value = model_logit.best_params_['C']\n",
    "        penalty_v = model_logit.best_params_['penalty']\n",
    "    \n",
    "        pars = {\"random_state\": 42, \"class_weight\": \"balanced\", \"n_jobs\": -1, \"solver\": \"saga\", \"max_iter\": 10000,\n",
    "                   \"C\": C_value, \"verbose\": 0, 'penalty': penalty_v}\n",
    "        lr_model = LogisticRegression(**pars)\n",
    "    \n",
    "    lr_model.fit(x_train,y_train)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decision_tree(x_train, y_train, criterion='entropy', max_depth=5, min_samples_leaf=20, GridSearch=False):\n",
    "    \n",
    "    dt_model = DecisionTreeClassifier(random_state=42, max_depth=max_depth,\n",
    "                                        min_samples_leaf=min_samples_leaf, class_weight=\"balanced\")\n",
    "    \n",
    "    if GridSearch:\n",
    "        parameters = dict(max_depth=list(range(3,7,1)), min_samples_leaf = list(range(5,41,5)),\n",
    "                         criterion  = ['gini', 'entropy'])\n",
    "        model_tree = GridSearchCV(dt_model, parameters, scoring='roc_auc', cv=3,verbose=5)\n",
    "\n",
    "        model_tree.fit(x_train,y_train)\n",
    "        print(model_tree.best_params_)\n",
    "        max_depth=model_tree.best_params_[\"max_depth\"]\n",
    "        min_samples_leaf=model_tree.best_params_[\"min_samples_leaf\"]\n",
    "        criterion = model_tree.best_params_[\"criterion\"]\n",
    "        \n",
    "        dt_model = DecisionTreeClassifier(random_state=42, max_depth=max_depth, criterion =criterion,\n",
    "                                        min_samples_leaf=min_samples_leaf, class_weight=\"balanced\")\n",
    "    \n",
    "    dt_model.fit(x_train,y_train)\n",
    "    return dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(x_train, y_train, max_depth=10, min_samples_leaf=20, n_estimators = 10,\n",
    "                      GridSearch=False):\n",
    "    \n",
    "    rf_model = RandomForestClassifier(random_state=42, max_depth=max_depth,n_jobs = -1, n_estimators=n_estimators,\n",
    "                                        min_samples_leaf=min_samples_leaf, class_weight=\"balanced\")\n",
    "\n",
    "    if GridSearch:\n",
    "        parameters = dict(max_depth=list(range(2,7,1)), min_samples_leaf = list(range(1,40,3)),\n",
    "                         n_estimators=list(range(10,100,20)))\n",
    "        model_rf = GridSearchCV(rf_model, parameters, scoring='roc_auc', verbose=5)\n",
    "\n",
    "        model_rf.fit(x_train,y_train)\n",
    "        print(model_rf.best_params_)\n",
    "        max_depth=model_rf.best_params_[\"max_depth\"]\n",
    "        min_samples_leaf=model_rf.best_params_[\"min_samples_leaf\"]\n",
    "        n_estimators=model_rf.best_params_[\"n_estimators\"]\n",
    "        \n",
    "        rf_model = RandomForestClassifier(random_state=42, max_depth=max_depth,n_estimators=n_estimators,\n",
    "                                        min_samples_leaf=min_samples_leaf, class_weight=\"balanced\")\n",
    "    \n",
    "    rf_model.fit(x_train,y_train)\n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(x_train, y_train):\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(x_train, y_train)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = fit_logistic_regression(x_train, y_train,  penalty_v = \"l1\", C_value = 0.01)\n",
    "report_metric(logistic_regression, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = fit_decision_tree(x_train, y_train,  criterion='entropy', max_depth=5, min_samples_leaf= 20, \n",
    "                                  GridSearch=True)\n",
    "\n",
    "report_metric(decision_tree, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest = fit_random_forest(x_train, y_train, GridSearch=False, max_depth= 6, min_samples_leaf= 1, n_estimators=90)\n",
    "report_metric(random_forest, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = fit_naive_bayes(x_train, y_train)\n",
    "report_metric(naive_bayes, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(x_train.shape[1], 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(cnn.summary())\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(x_train.values, (x_train.shape[0],x_train.shape[1],1))\n",
    "testX = np.reshape(x_test.values, (x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "cnn.fit(trainX, y_train, nb_epoch=20,validation_data=(testX, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_wrapper:\n",
    "    def __init__(self, model):\n",
    "        self._model= model\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        predictions = self._model.predict_proba(X)\n",
    "        return np.array(list(map(list,list(zip(1- predictions, predictions)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w = cnn_wrapper(cnn)\n",
    "report_metric(cnn_w, trainX, y_train, testX, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder\n",
    "X = df_sj.loc[:, ~df_sj.columns.isin([\"user_id\", \"fraud\"])]\n",
    "y = df_sj['fraud'].values\n",
    "X = X.replace(math.inf, 0)\n",
    "\n",
    "x_train_raw, x_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=41)\n",
    "x_train_filt = filter_correlated_features(x_train_raw)\n",
    "\n",
    "y_train_orig = y_train.copy()\n",
    "x_train_orig = x_train_filt.copy()\n",
    "\n",
    "x_train_interim = x_train_filt[y_train==0]\n",
    "y_train = y_train[y_train==0]\n",
    "\n",
    "x_train, scaler = standardize_data(x_train_interim)\n",
    "x_train_orig, _= standardize_data(adjust_features(x_train.columns, x_train_orig), scaler)\n",
    "x_test_filt = adjust_features(x_train.columns, x_test_raw)\n",
    "x_test, _ = standardize_data(x_test_filt, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 14\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, x_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(x_train_orig)\n",
    "mse = np.mean(np.power(x_train_orig - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_train_orig})\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)\n",
    "\n",
    "precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "plt.plot(th, recall[1:], 'r', label='Threshold-Precision curve')\n",
    "plt.title('Precision for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_wrapper:\n",
    "    def __init__(self, model):\n",
    "        self._model= model\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        predictions = self._model.predict(X)\n",
    "#         mse = np.mean(np.power(X - predictions, 2), axis=1)\n",
    "        mse = np.mean(np.absolute(X - predictions), axis=1)\n",
    "        return np.array(list(map(list,list(zip(1- mse.values, mse.values)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_wrapper = model_wrapper(autoencoder)\n",
    "autoencoder_wrapper.predict_proba(x_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metric(autoencoder_wrapper, x_train_orig, y_train_orig, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
